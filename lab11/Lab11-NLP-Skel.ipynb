{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelucrarea Limbajului Natural: Analiza Sentimentelor\n",
    " - Tudor Berariu\n",
    " - Andrei Olaru\n",
    "\n",
    "Scopul acestui laborator îl reprezintă rezolvarea unei probleme ce implică analiza unor documente în limbaj natural și învățarea unui algoritm simplu de clasificare: **Naive Bayes**.\n",
    "\n",
    "## Analiza Sentimentelor\n",
    "\n",
    "O serie de probleme de inteligență artificială presupun asocierea unei clase unui document în limbaj natural. Exemple de astfel de probleme sunt: **clasificarea** email-urilor în *spam* sau *ham* sau a recenziilor unor filme în *pozitive* sau *negative*. În laboratorul de astăzi vom aborda problema din urmă.\n",
    "\n",
    "Folosind setul de date de aici: http://www.cs.cornell.edu/people/pabo/movie-review-data/ (2000 de recenzii de film), vom construi un model care să discrimineze între recenziile pozitive și recenziile negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmul Naive Bayes\n",
    "\n",
    "### Clasificare\n",
    "\n",
    "Având un set de date $\\langle \\mathbf{X}, \\mathbf{T} \\rangle$ compus din $N$ exemple $\\mathbf{x}^{(i)}$, $1 \\le i \\le N$, descrise prin $k$ atribute $(x^{(i)}_1, x^{(i)}_2, \\ldots, x^{(i)}_k)$ și etichetate cu o clasă $t^{(i)} \\in \\mathcal{C}$, se cere construirea unui clasificator care să eticheteze exemple noi.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "**Naive Bayes** reprezintă o *metodă statistică inductivă* de clasificare, bazată pe Teorema lui Bayes pentru exprimarea relației dintre probabilitatea *a priori* și probabilitatea *posterioară* ale unei ipoteze.\n",
    "\n",
    "$$P(c \\vert \\mathbf{x}) = \\frac{P(\\mathbf{x} \\vert c) \\cdot P(c)}{P(\\mathbf{x})}$$\n",
    "\n",
    " - $P(c)$ reprezintă probabilitatea *a priori* a clasei $c$\n",
    " - $P(c \\vert \\mathbf{x})$ reprezintă probabilitatea *a posteriori* (după observarea lui $\\mathbf{x}$)\n",
    " - $P(\\mathbf{x} \\vert c)$ reprezitnă probabilitatea ca $\\mathbf{x}$ să aparțină clasei $c$ (*verosimilitatea*)\n",
    " \n",
    "Un clasificator **Naive Bayes** funcționează pe principiul verosimilității maxime (eng. *maximum likelihood*), deci alege clasa $c$ pentru care probabilitatea $P(c \\vert x)$ este maximă:\n",
    "\n",
    "$$c_{MAP} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} P(c \\vert \\mathbf{x}) = \\underset{c \\in \\mathcal{C}}{\\arg\\max} \\frac{P(\\mathbf{x} \\vert c) \\cdot P(c)}{P(x)} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} P(\\mathbf{x} \\vert c) \\cdot P(c)$$\n",
    "\n",
    "Cum fiecare exemplu $\\mathbf{x}$ este descris prin $K$ atribute:\n",
    "\n",
    "$$c_{MAP} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} P(x_1, x_2, \\ldots x_K \\vert c) \\cdot P(c)$$\n",
    "\n",
    "Algoritmul **Naive Bayes** face o presupunere simplificatoare, și anume, că atributele unui exemplu sunt *condițional independente* odată ce clasa este cunoscută:\n",
    "\n",
    "$$P(\\mathbf{x} \\vert c) = \\displaystyle\\prod_i P(x_i \\vert c)$$\n",
    "\n",
    "Astfel clasa pe care o prezice un clasificator **Naive Bayes** este:\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} P(c) \\cdot \\displaystyle \\prod_{i}^{K} P(x_i \\vert c)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificarea documentelor\n",
    "\n",
    "Pentru clasificare documentele vor fi reprezentate prin vectori binari de lungimea vocabularului (eng. *bag of words*). Practic fiecare document va avea 1 pe pozițiile corspunzătoare cuvintelor pe care le conține și 0 pe toate celelalte poziții. Dimensiunea unui exemplu $\\mathbf{x}$ este, deci, numărul de cuvinte diferite din setul de date.\n",
    "\n",
    "### Estimarea parametrilor modelului Naive Bayes\n",
    "\n",
    "Probabilitatea _a priori_ pentru o clasă $c \\in \\mathcal{C}$:\n",
    "\n",
    "$$P(c) = \\frac{\\#\\text{ docs in class }c}{\\#\\text{ total docs}}$$\n",
    "\n",
    "$P(x_i \\vert c)$ va reprezenta probabilitatea de a apărea cuvântul $x_i$ într-un document din clasa $c$ și o vom estima cu raportul dintre numărul de apariții ale cuvântului $x_i$ în documentele din clasa $c$ și numărul total de cuvinte ale acelor documente:\n",
    "\n",
    "$$P(x_i \\vert c) = \\frac{\\#\\text{ aparitii ale lui } x_i \\text{ in documente din clasa } c}{\\#\\text{ numar total de cuvinte in documentele din clasa } c}$$\n",
    "\n",
    "Deoarece este posibil ca un cuvant _rar_ ce apare într-un exemplu de test să nu se găsească deloc într-una din clase, se poate întâmpla ca un astfel de _accident_ să anuleze complet o probabilitate. Dacă un singur factor al unui produs este zero, atunci produsul devine zero. De aceea vom folosi netezire Laplace (eng. _Laplace smoothing_):\n",
    "\n",
    "$$P(x_i \\vert c) = \\frac{\\#\\text{ aparitii ale lui } x_i \\text{ in documente din clasa } c + \\alpha}{\\#\\text{ numar total de cuvinte in documentele din clasa } c + \\vert Voc \\vert \\cdot \\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setul de date\n",
    "\n",
    " 1. Descărcați setul de date **polarity dataset v2.0** de aici http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    " 2. Dezarhivați fișierul **review_polarity.tar.gz** și rearhivați directorul review_polarity ca zip.\n",
    " 3. Plasați / încărcați **review_polarity.zip** în directorul de lucru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recenzii pozitive: 1000; Recenzii negative: 1000\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zipFile = zipfile.ZipFile(\"review_polarity.zip\")\n",
    "\n",
    "pos_files = [f for f in zipFile.namelist() if '/pos/cv' in f]\n",
    "neg_files = [f for f in zipFile.namelist() if '/neg/cv' in f]\n",
    "\n",
    "pos_files.sort()\n",
    "neg_files.sort()\n",
    "\n",
    "print(\"Recenzii pozitive: \" + str(len(pos_files)) + \"; Recenzii negative: \" + str(len(neg_files)))\n",
    "\n",
    "# Raspunsul asteptat: \"Recenzii pozitive: 1000; Recenzii negative: 1000\"\n",
    "assert(len(pos_files) == 1000 and len(neg_files) == 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setul de antrenare și setul de testare\n",
    "\n",
    "Vom folosi 80% din datele din fiecare clasă pentru antrenare și 20% pentru testare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pos_no = int(.9 * len(pos_files))\n",
    "tr_neg_no = int(.9 * len(neg_files))\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(pos_files)\n",
    "shuffle(neg_files)\n",
    "\n",
    "pos_train = pos_files[:tr_pos_no] # Recenzii pozitive pentru antrenare\n",
    "pos_test  = pos_files[tr_pos_no:] # Recenzii pozitive pentru testare\n",
    "neg_train = neg_files[:tr_neg_no] # Recenzii negative pentru antrenare\n",
    "neg_test  = neg_files[tr_neg_no:] # Recenzii negative pentru testare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construirea vocabularului și calculul parametrilor\n",
    "\n",
    "Funcția `parse_document` primește calea către unul dinte fișierele aflate în arhivă și întoarce cuvintele din acest fișier (exceptând cuvintele cu o singură literă și pe cele din lista `STOP_WORDS`. Implementați funcția `count_words` astfel încât să întoarcă un dicționar cu o intrare pentru fiecare cuvânt care să conțină un tuplu cu două valori: numărul de apariții ale acelui cuvânt în rencezii pozitive și numărul de apariții în recenzii negative. În afara acelui dicționar se vor întoarce și numărul total de cuvinte din recenziile pozitive și numărul total de cuvinte din recenziile negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularul are  47171  cuvinte.\n",
      "330647  cuvinte in recenziile pozitive si  299178  cuvinte in recenziile negative\n",
      "Cuvantul 'beautiful' are  [180, 83]  aparitii.\n",
      "Cuvantul 'awful' are  [21, 102]  aparitii.\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = []\n",
    "STOP_WORDS = [line.strip() for line in open(\"Lab11-stop_words\")]\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_document(path):\n",
    "    for word in re.findall(r\"[-\\w']+\", zipFile.read(path).decode(\"utf-8\")):\n",
    "        if len(word) > 1 and word not in STOP_WORDS:\n",
    "            yield word\n",
    "\n",
    "def count_words():\n",
    "    vocabulary = {}\n",
    "    pos_words_no = 0\n",
    "    neg_words_no = 0\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # <TODO 1> numrati aparitiile in documente pozitive si\n",
    "    # in documente negative ale fiecarui cuvant, precum si numarul total\n",
    "    # de cuvinte din fiecare tip de recenzie\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    for d in pos_train:\n",
    "        for word in parse_document(d):\n",
    "            if word in vocabulary:\n",
    "                vocabulary[word][0] += 1\n",
    "            else:\n",
    "                vocabulary[word] = [1, 0]\n",
    "            pos_words_no += 1\n",
    "\n",
    "    for d in neg_train:\n",
    "        for word in parse_document(d):\n",
    "            if word in vocabulary:\n",
    "                vocabulary[word][1] += 1\n",
    "            else:\n",
    "                vocabulary[word] = [0, 1]\n",
    "            neg_words_no += 1\n",
    "    return (vocabulary, pos_words_no, neg_words_no)\n",
    "\n",
    "# -- VERIFICARE --\n",
    "training_result_words = count_words()\n",
    "\n",
    "(voc, p_no, n_no) = training_result_words\n",
    "print(\"Vocabularul are \", len(voc), \" cuvinte.\")\n",
    "print(p_no, \" cuvinte in recenziile pozitive si \", n_no, \" cuvinte in recenziile negative\")\n",
    "print(\"Cuvantul 'beautiful' are \", voc.get(\"beautiful\", (0, 0)), \" aparitii.\")\n",
    "print(\"Cuvantul 'awful' are \", voc.get(\"awful\", (0, 0)), \" aparitii.\")\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# Vocabularul are  44895  cuvinte.\n",
    "# 526267  cuvinte in recenziile pozitive si  469812  cuvinte in recenziile negative\n",
    "# Cuvantul 'beautiful' are  (165, 75)  aparitii.\n",
    "# Cuvantul 'awful' are  (16, 89)  aparitii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicția sentimentului unei recenzii noi\n",
    "\n",
    "Implementați funcția `predict` care primește parametrii `params` (vocabularul, numărul total de cuvinte din recenziile pozitive și numărul total de cuvinte din recenziile negative) și `path` (calea către o recenzie din cadrul arhivei) și întoarce clasa mai probabilă și logaritmul acelei probabilități. Al treilea argument (opțional) al funcției `predict` este coeficientul pentru netezire Laplace.\n",
    "\n",
    "Așa cum a fost explicat anterior, clasa pe care o prezice un clasificator **Naive Bayes** este dată de următoarea expresie:\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} P(c) \\cdot \\displaystyle \\prod_{i}^{K} P(x_i \\vert c)$$\n",
    "\n",
    "Pentru a evita lucrul cu numere foarte mici ce pot rezulta din produsul multor valori subunitare, vom logaritma expresiile date:\n",
    "\n",
    "$$c_{NB} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} \\log(P(c)) + \\displaystyle\\sum_{i}^{K} \\log(P(x_i \\vert c))$$\n",
    "\n",
    "Pentru calculul probabilitatilor, vedeti sectiunea \"Estimarea parametrilor modelului Naive Bayes\", mai sus. În cod, `log_pos` și `log_neg` trebuie însumate cu logaritmul pentru fiecare exemplu -- $ \\log(P(c)) $ este deja adunat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredients : london gal , fate , true love , running joke about monty python's spanish inquisition \n",
      "synopsis : american actress gwyneth paltrow with dark hair , playing a londoner ? \n",
      "believe it . \n",
      "sliding doors is a love 'what if' story . \n",
      "the gimmick is that it's really two stories : the film follows the life of helen ( gwyneth paltrow ) down two directions . \n",
      "in the beginning , likeable gal 'helen' ( gwyneth paltrow ) gets unexpectedly fired from her advertising job . \n",
      "so she goes into the subway train station meaning to return home early to her apartment and her sleeping live-in lover gerry ( john lynch ) . \n",
      "helen doesn't know it , but she is at a fateful junction in life : 1 ) if she enters through the sliding door of a london subway train her life takes one path with one future 2 ) if she stays on the platform her life takes another path with a different future . \n",
      "the film shows what happens in both paths , switching back and forth between intertwined , parallel stories . \n",
      "in story one , helen meets a charming and talkative monty python fan named james ( john hannah ) on the train . \n",
      "arriving home early , she discovers that her live-in , gerry ( john lynch ) , is having sex with his former lover lydia ( jeanne tripplehorn ) . \n",
      "this leads to a life where helen moves out . \n",
      "helen's winsome new friend james helps her recover from a broken heart , and encourages her to start her own business . \n",
      "in story two , helen experiences a different fate . \n",
      "she misses the train , never meets james , and doesn't get home early enough to discover gerry's infidelity . \n",
      "in this new life , helen takes up odd menial jobs , and faces the constant sneaking suspicion that all is not right with her relationship with gerry . \n",
      "will the truth of the heart finally work its way through a number of problems and setbacks in both scenarios ? \n",
      "opinion : rejoice all ye monty python fans , gwyneth paltrow fans , and watchers of quirky romance flicks . \n",
      "at last here's proof that there's still creativity in 1990s filmmaking . \n",
      "sliding doors is refreshingly different from anything this year . \n",
      "not only is it a well-acted , heartwarming film , but it's also easily gwyneth paltrow's best recent performance . \n",
      "when helen ( paltrow ) screams that her unfaithful boyfriend is a 'shagging wanker , ' ( a british phrase better left untranslated ) she sounds like she knows what she's talking about ! \n",
      "nor does sliding doors go overboard with formulaic true love or other stereotypes . \n",
      "nobody scrambles around in danger , screaming , \" i will find you no matter what , my darling ! \" \n",
      "when helen finds herself on a boat in the starlight with james , she doesn't take the easy way out and leap into his arms , since she's supposed to be recovering from heartache . \n",
      "and handsome live-in boyfriend gerry isn't the stereotypical screen hunk ( either a stud or a snob ) . \n",
      "instead he's a nervous and indecisive , almost helpless hunk . \n",
      "another example : when one of the protagonists lies wounded in a hospital , sliding doors gives us neither the 'hospital miracle' nor a maudlin tragedy but surprises us with a third variation . \n",
      "in other words , all of the characters seem non-stereotyped , human , and local . \n",
      "there are minor inconveniences . \n",
      "since sliding doors switches back and forth between two possible fates , it's occasionally difficult to distinguish between the two . \n",
      "distinguishing between the two stories isn't a problem in the scenes containing paltrow , who sports two different hairstyles . \n",
      "but in scenes containing only lydia and gerry , who look the same in both stories , it's slightly confusing . \n",
      "also , quite a few snappy comebacks referring to american pop culture ( seinfeld , woody allen , etc . ) are spoken by the british characters . \n",
      "but these seem slightly forced , given that the remaining dialogue is predominantly british slang . \n",
      "possibly an attempt by the screenwriter to balance the british so that american audiences can feel more comfortable ? \n",
      "sliding doors is a charming , quirky , original , happy romance with a little 'philosophy of fate' thrown in . \n",
      "monty python and the meaning of fate , anybody ? \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pos', -3556.9644026199926)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def predict(params, path, alpha = 1):\n",
    "    (vocabulary, pos_words_no, neg_words_no) = params\n",
    "    log_pos = log(0.5)\n",
    "    log_neg = log(0.5)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # <TODO 2> Calculul logaritmilor probabilităților\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    for word in parse_document(path):\n",
    "        if word in vocabulary:\n",
    "            log_pos += log((vocabulary[word][0] + alpha) / (pos_words_no + alpha * len(vocabulary)))\n",
    "            log_neg += log((vocabulary[word][1] + alpha) / (neg_words_no + alpha * len(vocabulary)))\n",
    "        else:\n",
    "            log_pos += log(alpha / (pos_words_no + alpha * len(vocabulary)))\n",
    "            log_neg += log(alpha / (neg_words_no + alpha * len(vocabulary)))\n",
    "    if log_pos > log_neg:\n",
    "        return \"pos\", log_pos\n",
    "    else:\n",
    "        return \"neg\", log_neg\n",
    "\n",
    "# -- VERIFICARE --\n",
    "print(zipFile.read(pos_test[14]).decode(\"utf-8\"))\n",
    "predict(training_result_words, pos_test[14])\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# ('pos', -1790.27088356391) pentru un film cu Hugh Grant și Julia Roberts (o mizerie siropoasă)\n",
    "#\n",
    "# Recenzia este clasificată corect ca fiind pozitivă."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluarea modelului\n",
    "\n",
    "Pentru a evalua modelul vom calcula acuratețea acestuia și matricea de confuzie, folosind datele de test (`pos_test` și `neg_test`).\n",
    "\n",
    "[Vedeți aici despre matricea de confuzie](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea pe setul de date de test:  86.5 %. Matricea de confuzie:\n",
      "    |     pos      |     neg    \n",
      "--- + ------------ + ------------\n",
      "pos |      89      |      11    \n",
      "neg |      16      |      84    \n"
     ]
    }
   ],
   "source": [
    "def evaluate(params, prediction_func):\n",
    "    conf_matrix = {}\n",
    "    conf_matrix[\"pos\"] = {\"pos\": 0, \"neg\": 0}\n",
    "    conf_matrix[\"neg\"] = {\"pos\": 0, \"neg\": 0}\n",
    "    accuracy = 0\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # <TODO 3> : Calcularea acurateței și a matricei de confuzie\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    for d in pos_test:\n",
    "        (prediction, score) = prediction_func(params, d)\n",
    "        if prediction == \"pos\":\n",
    "            conf_matrix[\"pos\"][\"pos\"] += 1\n",
    "        else:\n",
    "            conf_matrix[\"pos\"][\"neg\"] += 1\n",
    "    for d in neg_test:\n",
    "        (prediction, score) = prediction_func(params, d)\n",
    "        if prediction == \"neg\":\n",
    "            conf_matrix[\"neg\"][\"neg\"] += 1\n",
    "        else:\n",
    "            conf_matrix[\"neg\"][\"pos\"] += 1\n",
    "\n",
    "    accuracy = (conf_matrix[\"pos\"][\"pos\"] + conf_matrix[\"neg\"][\"neg\"]) / (conf_matrix[\"pos\"][\"pos\"] + conf_matrix[\"pos\"][\"neg\"] + conf_matrix[\"neg\"][\"pos\"] + conf_matrix[\"neg\"][\"neg\"])\n",
    "    return accuracy, conf_matrix\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    print(\"    | \", \"{0:^10}\".format(\"pos\"), \" | \", \"{0:^10}\".format(\"neg\"))\n",
    "    print(\"{0:-^3}\".format(\"\"), \"+\", \"{0:-^12}\".format(\"\"), \"+\", \"{0:-^12}\".format(\"-\", fill=\"-\"))\n",
    "    print(\"pos | \", \"{0:^10}\".format(cm[\"pos\"][\"pos\"]), \" | \", \"{0:^10}\".format(cm[\"pos\"][\"neg\"]))\n",
    "    print(\"neg | \", \"{0:^10}\".format(cm[\"neg\"][\"pos\"]), \" | \", \"{0:^10}\".format(cm[\"neg\"][\"neg\"]))\n",
    "\n",
    "\n",
    "# -- VERIFICARE --\n",
    "(acc_words, cm_words) = evaluate(training_result_words, predict)\n",
    "print(\"Acuratetea pe setul de date de test: \", acc_words * 100, \"%. Matricea de confuzie:\")\n",
    "print_confusion_matrix(cm_words)\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# Acuratetea pe setul de date de test:  80.5 %. Matricea de confuzie:\n",
    "#     |     pos      |     neg\n",
    "# --- + ------------ + ------------\n",
    "# pos |     155      |      45\n",
    "# neg |      33      |     167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Un model mai bun? Să folosim bigrame? Da!\n",
    "\n",
    "Implementați funcția `count_bigrams`, similară cu `count_words`, doar că de data aceasta dicționarul va conține bigramele din text. Funcția va întoarce tot trei elemente: dicționarul cu aparițiile în recenzii pozitive și în recenzii negative ale bigramelor, numărul total de bigrame din recenziile pozitive și numărul total de bigrame din recenziile negative.\n",
    "\n",
    "Salvați o bigramă prin concatenarea primului cuvânt, semnului \":\" și a celui de-al doilea cuvânt. De exemplu: `\"texas:ranger\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelul are  485151  bigrame.\n",
      "329747  bigrame in recenziile pozitive si  298278  bigrame in recenziile negative\n",
      "Bigrama 'beautiful actress' are  [2, 0]  aparitii.\n",
      "Bigrama 'awful movie' are  [1, 4]  aparitii.\n"
     ]
    }
   ],
   "source": [
    "def count_bigrams():\n",
    "    bigrams = {}\n",
    "    pos_bigrams_no = 0\n",
    "    neg_bigrams_no = 0\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # <TODO 4>: Numarati bigramele\n",
    "\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    for d in pos_train:\n",
    "        words = list(parse_document(d))\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = words[i] + \":\" + words[i + 1]\n",
    "            if bigram in bigrams:\n",
    "                bigrams[bigram][0] += 1\n",
    "            else:\n",
    "                bigrams[bigram] = [1, 0]\n",
    "            pos_bigrams_no += 1\n",
    "\n",
    "    for d in neg_train:\n",
    "        words = list(parse_document(d))\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = words[i] + \":\" + words[i + 1]\n",
    "            if bigram in bigrams:\n",
    "                bigrams[bigram][1] += 1\n",
    "            else:\n",
    "                bigrams[bigram] = [0, 1]\n",
    "            neg_bigrams_no += 1\n",
    "    return bigrams, pos_bigrams_no, neg_bigrams_no\n",
    "\n",
    "# -- VERIFICARE --\n",
    "training_result_bigrams = count_bigrams()\n",
    "\n",
    "(big, pos_b, neg_b) = training_result_bigrams\n",
    "print(\"Tabelul are \", len(big), \" bigrame.\")\n",
    "print(pos_b, \" bigrame in recenziile pozitive si \", neg_b, \" bigrame in recenziile negative\")\n",
    "print(\"Bigrama 'beautiful actress' are \", big.get(\"beautiful:actress\", (0, 0)), \" aparitii.\")\n",
    "print(\"Bigrama 'awful movie' are \", big.get(\"awful:movie\", (0, 0)), \" aparitii.\")\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# Tabelul are  428997  bigrame.\n",
    "# 525467  bigrame in recenziile pozitive si  469012  bigrame in recenziile negative\n",
    "# Bigrama 'beautiful actress' are  (2, 0)  aparitii.\n",
    "# Bigrama 'awful movie' are  (1, 4)  aparitii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcția de predicție folosind bigrame\n",
    "\n",
    "Implementați funcția `predict2` care să calculeze logaritmul probabilității fiecărei clase pe baza bigramelor din text. Trebuie să calculați `log_pos` și `log_neg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredients : london gal , fate , true love , running joke about monty python's spanish inquisition \n",
      "synopsis : american actress gwyneth paltrow with dark hair , playing a londoner ? \n",
      "believe it . \n",
      "sliding doors is a love 'what if' story . \n",
      "the gimmick is that it's really two stories : the film follows the life of helen ( gwyneth paltrow ) down two directions . \n",
      "in the beginning , likeable gal 'helen' ( gwyneth paltrow ) gets unexpectedly fired from her advertising job . \n",
      "so she goes into the subway train station meaning to return home early to her apartment and her sleeping live-in lover gerry ( john lynch ) . \n",
      "helen doesn't know it , but she is at a fateful junction in life : 1 ) if she enters through the sliding door of a london subway train her life takes one path with one future 2 ) if she stays on the platform her life takes another path with a different future . \n",
      "the film shows what happens in both paths , switching back and forth between intertwined , parallel stories . \n",
      "in story one , helen meets a charming and talkative monty python fan named james ( john hannah ) on the train . \n",
      "arriving home early , she discovers that her live-in , gerry ( john lynch ) , is having sex with his former lover lydia ( jeanne tripplehorn ) . \n",
      "this leads to a life where helen moves out . \n",
      "helen's winsome new friend james helps her recover from a broken heart , and encourages her to start her own business . \n",
      "in story two , helen experiences a different fate . \n",
      "she misses the train , never meets james , and doesn't get home early enough to discover gerry's infidelity . \n",
      "in this new life , helen takes up odd menial jobs , and faces the constant sneaking suspicion that all is not right with her relationship with gerry . \n",
      "will the truth of the heart finally work its way through a number of problems and setbacks in both scenarios ? \n",
      "opinion : rejoice all ye monty python fans , gwyneth paltrow fans , and watchers of quirky romance flicks . \n",
      "at last here's proof that there's still creativity in 1990s filmmaking . \n",
      "sliding doors is refreshingly different from anything this year . \n",
      "not only is it a well-acted , heartwarming film , but it's also easily gwyneth paltrow's best recent performance . \n",
      "when helen ( paltrow ) screams that her unfaithful boyfriend is a 'shagging wanker , ' ( a british phrase better left untranslated ) she sounds like she knows what she's talking about ! \n",
      "nor does sliding doors go overboard with formulaic true love or other stereotypes . \n",
      "nobody scrambles around in danger , screaming , \" i will find you no matter what , my darling ! \" \n",
      "when helen finds herself on a boat in the starlight with james , she doesn't take the easy way out and leap into his arms , since she's supposed to be recovering from heartache . \n",
      "and handsome live-in boyfriend gerry isn't the stereotypical screen hunk ( either a stud or a snob ) . \n",
      "instead he's a nervous and indecisive , almost helpless hunk . \n",
      "another example : when one of the protagonists lies wounded in a hospital , sliding doors gives us neither the 'hospital miracle' nor a maudlin tragedy but surprises us with a third variation . \n",
      "in other words , all of the characters seem non-stereotyped , human , and local . \n",
      "there are minor inconveniences . \n",
      "since sliding doors switches back and forth between two possible fates , it's occasionally difficult to distinguish between the two . \n",
      "distinguishing between the two stories isn't a problem in the scenes containing paltrow , who sports two different hairstyles . \n",
      "but in scenes containing only lydia and gerry , who look the same in both stories , it's slightly confusing . \n",
      "also , quite a few snappy comebacks referring to american pop culture ( seinfeld , woody allen , etc . ) are spoken by the british characters . \n",
      "but these seem slightly forced , given that the remaining dialogue is predominantly british slang . \n",
      "possibly an attempt by the screenwriter to balance the british so that american audiences can feel more comfortable ? \n",
      "sliding doors is a charming , quirky , original , happy romance with a little 'philosophy of fate' thrown in . \n",
      "monty python and the meaning of fate , anybody ? \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pos', -5231.050349268755)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict2(params, path, alpha = 1):\n",
    "    (bigrams, pos_bigrams_no, neg_bigrams_no) = params\n",
    "    log_pos = log(0.5)\n",
    "    log_neg = log(0.5)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # <TODO 5> Calculul logaritmilor probabilităților folosind bigramele\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    words = list(parse_document(path))\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = words[i] + \":\" + words[i + 1]\n",
    "        if bigram in bigrams:\n",
    "            log_pos += log((bigrams[bigram][0] + alpha) / (pos_bigrams_no + alpha * len(bigrams)))\n",
    "            log_neg += log((bigrams[bigram][1] + alpha) / (neg_bigrams_no + alpha * len(bigrams)))\n",
    "        else:\n",
    "            log_pos += log(alpha / (pos_bigrams_no + alpha * len(bigrams)))\n",
    "            log_neg += log(alpha / (neg_bigrams_no + alpha * len(bigrams)))\n",
    "\n",
    "\n",
    "    if log_pos > log_neg:\n",
    "        return \"pos\", log_pos\n",
    "    else:\n",
    "        return \"neg\", log_neg\n",
    "\n",
    "# -- VERIFICARE --\n",
    "print(zipFile.read(pos_test[14]).decode(\"utf-8\"))\n",
    "predict2(training_result_bigrams, pos_test[14])\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# ('pos', -3034.428732037113) pentru același film cu Hugh Grant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea pe setul de date de test, cu bigrame:  78.5 %. Matricea de confuzie:\n",
      "    |     pos      |     neg    \n",
      "--- + ------------ + ------------\n",
      "pos |      65      |      35    \n",
      "neg |      8       |      92    \n"
     ]
    }
   ],
   "source": [
    "# -- VERIFICARE --\n",
    "(acc_bigrams, cm_bigrams) = evaluate(training_result_bigrams, predict2)\n",
    "print(\"Acuratetea pe setul de date de test, cu bigrame: \", acc_bigrams * 100, \"%. Matricea de confuzie:\")\n",
    "print_confusion_matrix(cm_bigrams)\n",
    "\n",
    "# Daca se comentează liniile care reordonează aleator listele cu exemplele pozitive și negative,\n",
    "# rezultatul așteptat este:\n",
    "#\n",
    "# Acuratetea pe setul de date de test:  84.5 %. Matricea de confuzie:\n",
    "#     |     pos      |     neg\n",
    "# --- + ------------ + ------------\n",
    "# pos |     161      |      39\n",
    "# neg |      23      |     177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La final...\n",
    "\n",
    " 1. Decomentați liniile care reordonează aleator listele cu exemplele pozitive și cele negative (secțiunea \"Setul de antrenare și setul de testare\"). Rulați de mai multe ori. Este întotdeauna mai bun modelul cu bigrame? Acuratețea variază mult de la o rulare la alta?\n",
    " 2. Încercați să eliminați cuvintele de legătură (linia cu `STOP_WORDS`, din secțiunea \"Construirea vocabularului...\"). Ce impact are asupra performanței celor două modele?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea pe setul de date de test, cu cuvinte simple:  86.5 %. Matricea de confuzie:\n",
      "    |     pos      |     neg    \n",
      "--- + ------------ + ------------\n",
      "pos |      89      |      11    \n",
      "neg |      16      |      84    \n",
      "\n",
      "\n",
      "Acuratetea pe setul de date de test, cu bigrame:  78.5 %. Matricea de confuzie:\n",
      "    |     pos      |     neg    \n",
      "--- + ------------ + ------------\n",
      "pos |      65      |      35    \n",
      "neg |      8       |      92    \n"
     ]
    }
   ],
   "source": [
    "print(\"Acuratetea pe setul de date de test, cu cuvinte simple: \", acc_words * 100, \"%. Matricea de confuzie:\")\n",
    "print_confusion_matrix(cm_words)\n",
    "\n",
    "print(\"\\n\\nAcuratetea pe setul de date de test, cu bigrame: \", acc_bigrams * 100, \"%. Matricea de confuzie:\")\n",
    "print_confusion_matrix(cm_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
